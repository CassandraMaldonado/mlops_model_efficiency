<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>a4a23924ecaa4963a24c7b2ed1043bea</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      {   }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ff0000; font-weight: bold; } /* Alert */
    code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
    code span.at { color: #7d9029; } /* Attribute */
    code span.bn { color: #40a070; } /* BaseN */
    code span.bu { color: #008000; } /* BuiltIn */
    code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #4070a0; } /* Char */
    code span.cn { color: #880000; } /* Constant */
    code span.co { color: #60a0b0; font-style: italic; } /* Comment */
    code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
    code span.do { color: #ba2121; font-style: italic; } /* Documentation */
    code span.dt { color: #902000; } /* DataType */
    code span.dv { color: #40a070; } /* DecVal */
    code span.er { color: #ff0000; font-weight: bold; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #40a070; } /* Float */
    code span.fu { color: #06287e; } /* Function */
    code span.im { color: #008000; font-weight: bold; } /* Import */
    code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
    code span.kw { color: #007020; font-weight: bold; } /* Keyword */
    code span.op { color: #666666; } /* Operator */
    code span.ot { color: #007020; } /* Other */
    code span.pp { color: #bc7a00; } /* Preprocessor */
    code span.sc { color: #4070a0; } /* SpecialChar */
    code span.ss { color: #bb6688; } /* SpecialString */
    code span.st { color: #4070a0; } /* String */
    code span.va { color: #19177c; } /* Variable */
    code span.vs { color: #4070a0; } /* VerbatimString */
    code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<div id="4224577f" class="cell code" data-execution_count="1">
<div class="sourceCode" id="cb1"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> copy</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> types</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.optim <span class="im">as</span> optim</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.quantization <span class="im">as</span> tq</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torchvision.transforms <span class="im">as</span> transforms</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision <span class="im">import</span> models <span class="im">as</span> torchvision_models</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> torchvision.models.resnet <span class="im">import</span> BasicBlock</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span></code></pre></div>
</div>
<div id="47e34512" class="cell code">
<div class="sourceCode" id="cb2"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Settings</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    device_train <span class="op">=</span> torch.device(<span class="st">&#39;mps&#39;</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Using MPS for FP32 training.&quot;</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    device_train <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;Using CPU for training.&quot;</span>)</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="co"># CPU for quantization operations.</span></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>device_quant <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Using CPU for quantization operations.&quot;</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Quantization backend to qnnpack.</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>torch.backends.quantized.engine <span class="op">=</span> <span class="st">&#39;qnnpack&#39;</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Using qnnpack backend.&quot;</span>)</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>BATCH_SIZE <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>EPOCHS_BASELINE <span class="op">=</span> <span class="dv">10</span>  </span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>EPOCHS_QAT <span class="op">=</span> <span class="dv">3</span>       </span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>EPOCHS_KD <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>PRINT_FREQ <span class="op">=</span> <span class="dv">100</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a>os.makedirs(<span class="st">&#39;./quantization_results&#39;</span>, exist_ok<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Using MPS for FP32 training.
Using CPU for quantization operations.
Using qnnpack backend.
</code></pre>
</div>
</div>
<div id="458c9d54" class="cell code">
<div class="sourceCode" id="cb4"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Data: CIFAR-10</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>transform_train <span class="op">=</span> transforms.Compose([</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    transforms.RandomCrop(<span class="dv">32</span>, padding<span class="op">=</span><span class="dv">4</span>),</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    transforms.RandomHorizontalFlip(),</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.4914</span>, <span class="fl">0.4822</span>, <span class="fl">0.4465</span>), (<span class="fl">0.2023</span>, <span class="fl">0.1994</span>, <span class="fl">0.2010</span>)),</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>transform_test <span class="op">=</span> transforms.Compose([</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    transforms.ToTensor(),</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    transforms.Normalize((<span class="fl">0.4914</span>, <span class="fl">0.4822</span>, <span class="fl">0.4465</span>), (<span class="fl">0.2023</span>, <span class="fl">0.1994</span>, <span class="fl">0.2010</span>)),</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>trainset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">&#39;./data&#39;</span>, train<span class="op">=</span><span class="va">True</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform_train</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>trainloader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>    trainset, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">True</span>, num_workers<span class="op">=</span><span class="dv">2</span></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>testset <span class="op">=</span> torchvision.datasets.CIFAR10(</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    root<span class="op">=</span><span class="st">&#39;./data&#39;</span>, train<span class="op">=</span><span class="va">False</span>, download<span class="op">=</span><span class="va">True</span>, transform<span class="op">=</span>transform_test</span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>testloader <span class="op">=</span> torch.utils.data.DataLoader(</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    testset, batch_size<span class="op">=</span>BATCH_SIZE, shuffle<span class="op">=</span><span class="va">False</span>, num_workers<span class="op">=</span><span class="dv">2</span></span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Training samples: </span><span class="sc">{</span><span class="bu">len</span>(trainset)<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Test samples: </span><span class="sc">{</span><span class="bu">len</span>(testset)<span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Training samples: 50000
Test samples: 10000
</code></pre>
</div>
</div>
<div id="75a7b4b0" class="cell code">
<div class="sourceCode" id="cb6"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Model</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># ResNet18 configured for CIFAR-10.</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_resnet18(num_classes<span class="op">=</span><span class="dv">10</span>, pretrained<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> torchvision_models.resnet18(weights<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    model.conv1 <span class="op">=</span> nn.Conv2d(<span class="dv">3</span>, <span class="dv">64</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, stride<span class="op">=</span><span class="dv">1</span>, padding<span class="op">=</span><span class="dv">1</span>, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    model.maxpool <span class="op">=</span> nn.Identity()</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    model.fc <span class="op">=</span> nn.Linear(model.fc.in_features, num_classes)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    prepare_residual_blocks_for_quant(model)</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Quantization stubs.</span></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> QuantStubWrapper(nn.Module):</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, module: nn.Module):</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.module <span class="op">=</span> module</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.quant <span class="op">=</span> tq.QuantStub()</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.dequant <span class="op">=</span> tq.DeQuantStub()</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.quant(x)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.module(x)</span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> <span class="va">self</span>.dequant(x)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> x</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Making sure the model exposes Quant/DeQuant stubs exactly once.</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> wrap_with_quant_stubs(model: nn.Module) <span class="op">-&gt;</span> nn.Module:</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">isinstance</span>(model, QuantStubWrapper):</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> model</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> QuantStubWrapper(model)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> _basicblock_forward_quant(<span class="va">self</span>, x):</span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a>    identity <span class="op">=</span> x</span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="va">self</span>.conv1(x)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="va">self</span>.bn1(out)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="va">self</span>.conv2(out)</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="va">self</span>.bn2(out)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.downsample <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>        identity <span class="op">=</span> <span class="va">self</span>.downsample(x)</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="va">self</span>.skip_add.add(out, identity)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    out <span class="op">=</span> <span class="va">self</span>.relu(out)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> out</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare residual blocks for quantization.</span></span>
<span id="cb6-57"><a href="#cb6-57" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prepare_residual_blocks_for_quant(model: nn.Module) <span class="op">-&gt;</span> nn.Module:</span>
<span id="cb6-58"><a href="#cb6-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> module <span class="kw">in</span> model.modules():</span>
<span id="cb6-59"><a href="#cb6-59" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, BasicBlock) <span class="kw">and</span> <span class="kw">not</span> <span class="bu">hasattr</span>(module, <span class="st">&#39;skip_add&#39;</span>):</span>
<span id="cb6-60"><a href="#cb6-60" aria-hidden="true" tabindex="-1"></a>            module.skip_add <span class="op">=</span> nn.quantized.FloatFunctional()</span>
<span id="cb6-61"><a href="#cb6-61" aria-hidden="true" tabindex="-1"></a>            module.forward <span class="op">=</span> types.MethodType(_basicblock_forward_quant, module)</span>
<span id="cb6-62"><a href="#cb6-62" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model</span>
<span id="cb6-63"><a href="#cb6-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-64"><a href="#cb6-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-65"><a href="#cb6-65" aria-hidden="true" tabindex="-1"></a><span class="co"># Fuse Conv+BN+ReLU modules in ResNet18 for better quantization.</span></span>
<span id="cb6-66"><a href="#cb6-66" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fuse_resnet18_modules(model):</span>
<span id="cb6-67"><a href="#cb6-67" aria-hidden="true" tabindex="-1"></a>    fused_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-68"><a href="#cb6-68" aria-hidden="true" tabindex="-1"></a>    target_model <span class="op">=</span> <span class="bu">getattr</span>(model, <span class="st">&#39;module&#39;</span>, model)</span>
<span id="cb6-69"><a href="#cb6-69" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> module_name <span class="kw">in</span> [<span class="st">&#39;layer1&#39;</span>, <span class="st">&#39;layer2&#39;</span>, <span class="st">&#39;layer3&#39;</span>, <span class="st">&#39;layer4&#39;</span>]:</span>
<span id="cb6-70"><a href="#cb6-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="bu">hasattr</span>(target_model, module_name):</span>
<span id="cb6-71"><a href="#cb6-71" aria-hidden="true" tabindex="-1"></a>            layer <span class="op">=</span> <span class="bu">getattr</span>(target_model, module_name)</span>
<span id="cb6-72"><a href="#cb6-72" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> block <span class="kw">in</span> layer:</span>
<span id="cb6-73"><a href="#cb6-73" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">hasattr</span>(block, <span class="st">&#39;conv1&#39;</span>) <span class="kw">and</span> <span class="bu">hasattr</span>(block, <span class="st">&#39;bn1&#39;</span>):</span>
<span id="cb6-74"><a href="#cb6-74" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">try</span>:</span>
<span id="cb6-75"><a href="#cb6-75" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">if</span> <span class="bu">hasattr</span>(block, <span class="st">&#39;relu&#39;</span>):</span>
<span id="cb6-76"><a href="#cb6-76" aria-hidden="true" tabindex="-1"></a>                            tq.fuse_modules(block, [<span class="st">&#39;conv1&#39;</span>, <span class="st">&#39;bn1&#39;</span>, <span class="st">&#39;relu&#39;</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-77"><a href="#cb6-77" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">else</span>:</span>
<span id="cb6-78"><a href="#cb6-78" aria-hidden="true" tabindex="-1"></a>                            tq.fuse_modules(block, [<span class="st">&#39;conv1&#39;</span>, <span class="st">&#39;bn1&#39;</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-79"><a href="#cb6-79" aria-hidden="true" tabindex="-1"></a>                        fused_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-80"><a href="#cb6-80" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb6-81"><a href="#cb6-81" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">pass</span></span>
<span id="cb6-82"><a href="#cb6-82" aria-hidden="true" tabindex="-1"></a>                </span>
<span id="cb6-83"><a href="#cb6-83" aria-hidden="true" tabindex="-1"></a>                <span class="cf">if</span> <span class="bu">hasattr</span>(block, <span class="st">&#39;conv2&#39;</span>) <span class="kw">and</span> <span class="bu">hasattr</span>(block, <span class="st">&#39;bn2&#39;</span>):</span>
<span id="cb6-84"><a href="#cb6-84" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">try</span>:</span>
<span id="cb6-85"><a href="#cb6-85" aria-hidden="true" tabindex="-1"></a>                        tq.fuse_modules(block, [<span class="st">&#39;conv2&#39;</span>, <span class="st">&#39;bn2&#39;</span>], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb6-86"><a href="#cb6-86" aria-hidden="true" tabindex="-1"></a>                        fused_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb6-87"><a href="#cb6-87" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">except</span> <span class="pp">Exception</span>:</span>
<span id="cb6-88"><a href="#cb6-88" aria-hidden="true" tabindex="-1"></a>                        <span class="cf">pass</span></span>
<span id="cb6-89"><a href="#cb6-89" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> fused_count</span></code></pre></div>
</div>
<div id="eb1e7aa3" class="cell code">
<div class="sourceCode" id="cb7"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Utility functions</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluating the model on the specified device.</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_on_device(model, dataloader, device):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    model.to(device)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> x, y <span class="kw">in</span> dataloader:</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>            x, y <span class="op">=</span> x.to(device), y.to(device)</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(x)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>            _, predicted <span class="op">=</span> outputs.<span class="bu">max</span>(<span class="dv">1</span>)</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>            total <span class="op">+=</span> y.size(<span class="dv">0</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (predicted <span class="op">==</span> y).<span class="bu">sum</span>().item()</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fl">100.0</span> <span class="op">*</span> correct <span class="op">/</span> total</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluating the model on CPU.</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_cpu(model, dataloader):</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> evaluate_on_device(model, dataloader, torch.device(<span class="st">&#39;cpu&#39;</span>))</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Measuring the latency of the model.</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> measure_latency(model, device_type<span class="op">=</span><span class="st">&#39;cpu&#39;</span>, num_runs<span class="op">=</span><span class="dv">200</span>, input_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">32</span>, <span class="dv">32</span>)):</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    device_local <span class="op">=</span> torch.device(device_type)</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> model.to(device_local)</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>    dummy_input <span class="op">=</span> torch.randn(input_size).to(device_local)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>    <span class="co"># warmup.</span></span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> model(dummy_input)</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>    <span class="co"># measure.</span></span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>    times <span class="op">=</span> []</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_runs):</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>            start <span class="op">=</span> time.time()</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>            _ <span class="op">=</span> model(dummy_input)</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> device_type <span class="op">==</span> <span class="st">&#39;mps&#39;</span>:</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>                torch.mps.synchronize()</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>            times.append((time.time() <span class="op">-</span> start) <span class="op">*</span> <span class="dv">1000</span>)</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">sum</span>(times) <span class="op">/</span> <span class="bu">len</span>(times)</span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_model_size_mb(filepath):</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> os.path.getsize(filepath) <span class="op">/</span> (<span class="dv">1024</span> <span class="op">*</span> <span class="dv">1024</span>)</span></code></pre></div>
</div>
<div id="71e51f1b" class="cell code" data-execution_count="9">
<div class="sourceCode" id="cb8"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 1) Baseline: Train ResNet-18 (FP32)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;1) Baseline: Train ResNet-18 (FP32)&quot;</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>baseline <span class="op">=</span> get_resnet18().to(device_train)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Parameters: </span><span class="sc">{</span><span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> baseline.parameters())<span class="op">/</span><span class="fl">1e6</span><span class="sc">:.2f}</span><span class="ss">M&quot;</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Training on: </span><span class="sc">{</span>device_train<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>optimizer <span class="op">=</span> optim.SGD(baseline.parameters(), lr<span class="op">=</span><span class="fl">0.1</span>, momentum<span class="op">=</span><span class="fl">0.9</span>, weight_decay<span class="op">=</span><span class="fl">5e-4</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>scheduler <span class="op">=</span> optim.lr_scheduler.MultiStepLR(optimizer, milestones<span class="op">=</span>[<span class="dv">5</span>, <span class="dv">8</span>], gamma<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>criterion <span class="op">=</span> nn.CrossEntropyLoss()</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Training for </span><span class="sc">{</span>EPOCHS_BASELINE<span class="sc">}</span><span class="ss"> epochs.&quot;</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS_BASELINE):</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    baseline.train()</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (inputs, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader):</span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>        inputs, labels <span class="op">=</span> inputs.to(device_train), labels.to(device_train)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>        optimizer.zero_grad()</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> baseline(inputs)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>        optimizer.step()</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>        _, predicted <span class="op">=</span> outputs.<span class="bu">max</span>(<span class="dv">1</span>)</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a>        correct <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> PRINT_FREQ <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&#39;  Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>EPOCHS_BASELINE<span class="sc">}</span><span class="ss">, Step </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, &#39;</span></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>                  <span class="ss">f&#39;Loss: </span><span class="sc">{</span>running_loss<span class="op">/</span>PRINT_FREQ<span class="sc">:.4f}</span><span class="ss">, &#39;</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>                  <span class="ss">f&#39;Train Acc: </span><span class="sc">{</span><span class="fl">100.</span><span class="op">*</span>correct<span class="op">/</span>total<span class="sc">:.2f}</span><span class="ss">%&#39;</span>)</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    scheduler.step()</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> evaluate_on_device(baseline, testloader, device_train)</span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss"> complete. Test Accuracy: </span><span class="sc">{</span>acc<span class="sc">:.2f}</span><span class="ss">%&#39;</span>)</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a>acc_fp32 <span class="op">=</span> evaluate_on_device(baseline, testloader, device_train)</span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a>lat_fp32 <span class="op">=</span> measure_latency(baseline, <span class="st">&#39;mps&#39;</span> <span class="cf">if</span> device_train.<span class="bu">type</span> <span class="op">==</span> <span class="st">&#39;mps&#39;</span> <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a>torch.save(baseline.state_dict(), <span class="st">&#39;./quantization_results/resnet18_fp32.pth&#39;</span>)</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>size_fp32 <span class="op">=</span> get_model_size_mb(<span class="st">&#39;./quantization_results/resnet18_fp32.pth&#39;</span>)</span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;FP32 Baseline | Acc: </span><span class="sc">{</span>acc_fp32<span class="sc">:.2f}</span><span class="ss">%, Latency: </span><span class="sc">{</span>lat_fp32<span class="sc">:.2f}</span><span class="ss"> ms, Size: </span><span class="sc">{</span>size_fp32<span class="sc">:.2f}</span><span class="ss"> MB&quot;</span>)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>1) Baseline: Train ResNet-18 (FP32)
Parameters: 11.17M
Training on: mps

Training for 10 epochs.
  Epoch 1/10, Step 100, Loss: 2.7684, Train Acc: 14.68%
  Epoch 1/10, Step 200, Loss: 2.0101, Train Acc: 19.29%
  Epoch 1/10, Step 300, Loss: 1.8086, Train Acc: 23.41%
Epoch 1 complete. Test Accuracy: 37.48%
  Epoch 2/10, Step 100, Loss: 1.5786, Train Acc: 41.49%
  Epoch 2/10, Step 200, Loss: 1.5049, Train Acc: 43.29%
  Epoch 2/10, Step 300, Loss: 1.4435, Train Acc: 44.43%
Epoch 2 complete. Test Accuracy: 53.86%
  Epoch 3/10, Step 100, Loss: 1.2544, Train Acc: 53.95%
  Epoch 3/10, Step 200, Loss: 1.1634, Train Acc: 55.97%
  Epoch 3/10, Step 300, Loss: 1.1244, Train Acc: 57.17%
Epoch 3 complete. Test Accuracy: 60.27%
  Epoch 4/10, Step 100, Loss: 0.9952, Train Acc: 64.48%
  Epoch 4/10, Step 200, Loss: 0.9349, Train Acc: 65.75%
  Epoch 4/10, Step 300, Loss: 0.8983, Train Acc: 66.65%
Epoch 4 complete. Test Accuracy: 58.93%
  Epoch 5/10, Step 100, Loss: 0.7935, Train Acc: 71.90%
  Epoch 5/10, Step 200, Loss: 0.7854, Train Acc: 72.24%
  Epoch 5/10, Step 300, Loss: 0.7461, Train Acc: 72.97%
Epoch 5 complete. Test Accuracy: 71.82%
  Epoch 6/10, Step 100, Loss: 0.5667, Train Acc: 80.83%
  Epoch 6/10, Step 200, Loss: 0.5365, Train Acc: 80.98%
  Epoch 6/10, Step 300, Loss: 0.5136, Train Acc: 81.38%
Epoch 6 complete. Test Accuracy: 82.58%
  Epoch 7/10, Step 100, Loss: 0.4796, Train Acc: 82.88%
  Epoch 7/10, Step 200, Loss: 0.4788, Train Acc: 83.04%
  Epoch 7/10, Step 300, Loss: 0.4567, Train Acc: 83.45%
Epoch 7 complete. Test Accuracy: 83.80%
  Epoch 8/10, Step 100, Loss: 0.4480, Train Acc: 84.29%
  Epoch 8/10, Step 200, Loss: 0.4441, Train Acc: 84.43%
  Epoch 8/10, Step 300, Loss: 0.4254, Train Acc: 84.59%
Epoch 8 complete. Test Accuracy: 84.17%
  Epoch 9/10, Step 100, Loss: 0.3987, Train Acc: 86.19%
  Epoch 9/10, Step 200, Loss: 0.3970, Train Acc: 86.29%
  Epoch 9/10, Step 300, Loss: 0.3859, Train Acc: 86.45%
Epoch 9 complete. Test Accuracy: 85.15%
  Epoch 10/10, Step 100, Loss: 0.3868, Train Acc: 86.34%
  Epoch 10/10, Step 200, Loss: 0.3889, Train Acc: 86.41%
  Epoch 10/10, Step 300, Loss: 0.3928, Train Acc: 86.36%
Epoch 10 complete. Test Accuracy: 85.40%
----------------------------------------------------------------------
FP32 Baseline | Acc: 85.40%, Latency: 2.75 ms, Size: 42.70 MB
----------------------------------------------------------------------
</code></pre>
</div>
</div>
<div id="f5823dec" class="cell code" data-execution_count="10">
<div class="sourceCode" id="cb10"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 2) Post-Training Quantization (Dynamic Quantization)</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">#    Note: dynamic quantization mainly benefits linear layers (NLP) and may be limited for conv-heavy CNNs.</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">#    We&#39;ll demonstrate torch.quantization.quantize_dynamic and also a static quantization approach.</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;2) Post-Training Quantization (Dynamic Quantization)&quot;</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>model_dyn <span class="op">=</span> copy.deepcopy(baseline).cpu()</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>model_dyn.<span class="bu">eval</span>()</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>model_dyn_q <span class="op">=</span> tq.quantize_dynamic(</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    model_dyn,</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    {nn.Linear, nn.Conv2d},</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    dtype<span class="op">=</span>torch.qint8</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>acc_dyn <span class="op">=</span> evaluate_cpu(model_dyn_q, testloader)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a>lat_dyn <span class="op">=</span> measure_latency(model_dyn_q, <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>torch.save(model_dyn_q.state_dict(), <span class="st">&#39;./quantization_results/resnet18_dynamic.pth&#39;</span>)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>size_dyn <span class="op">=</span> get_model_size_mb(<span class="st">&#39;./quantization_results/resnet18_dynamic.pth&#39;</span>)</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Dynamic-PTQ | Acc: </span><span class="sc">{</span>acc_dyn<span class="sc">:.2f}</span><span class="ss">%, Latency: </span><span class="sc">{</span>lat_dyn<span class="sc">:.2f}</span><span class="ss"> ms, Size: </span><span class="sc">{</span>size_dyn<span class="sc">:.2f}</span><span class="ss"> MB&quot;</span>)</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>2) Post-Training Quantization (Dynamic Quantization)
</code></pre>
</div>
<div class="output stream stderr">
<pre><code>[W1117 00:04:33.331596000 qlinear_dynamic.cpp:252] Warning: Currently, qnnpack incorrectly ignores reduce_range when it is set to true; this may change in a future release. (function operator())
</code></pre>
</div>
<div class="output stream stdout">
<pre><code>----------------------------------------------------------------------
Dynamic-PTQ | Acc: 85.41%, Latency: 4.77 ms, Size: 42.69 MB
----------------------------------------------------------------------
</code></pre>
</div>
</div>
<div id="46794e22" class="cell code" data-execution_count="11">
<div class="sourceCode" id="cb14"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Static Post-Training Quantization</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;3. Static Post-Training Quantization&quot;</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>model_static <span class="op">=</span> wrap_with_quant_stubs(copy.deepcopy(baseline).cpu())</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>model_static.<span class="bu">eval</span>()</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Fuse modules.</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>fused <span class="op">=</span> fuse_resnet18_modules(model_static)</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Fused </span><span class="sc">{</span>fused<span class="sc">}</span><span class="ss"> module groups.&quot;</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting qconfig for qnnpack.</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>model_static.qconfig <span class="op">=</span> tq.get_default_qconfig(<span class="st">&#39;qnnpack&#39;</span>)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Using qconfig: </span><span class="sc">{</span>model_static<span class="sc">.</span>qconfig<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>tq.prepare(model_static, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Model preped for quantization.&quot;</span>)</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Calibration.</span></span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">Calibrating with 100 batches.&quot;</span>)</span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (x, _) <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader):</span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">100</span>:</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.cpu()</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> model_static(x)</span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb14-31"><a href="#cb14-31" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">25</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb14-32"><a href="#cb14-32" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&quot;  Calibrated </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/100 batches.&quot;</span>)</span>
<span id="cb14-33"><a href="#cb14-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-34"><a href="#cb14-34" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Calibration finished.&quot;</span>)</span>
<span id="cb14-35"><a href="#cb14-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-36"><a href="#cb14-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Converting.</span></span>
<span id="cb14-37"><a href="#cb14-37" aria-hidden="true" tabindex="-1"></a>tq.convert(model_static, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-38"><a href="#cb14-38" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Model converted to INT8.&quot;</span>)</span>
<span id="cb14-39"><a href="#cb14-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-40"><a href="#cb14-40" aria-hidden="true" tabindex="-1"></a>acc_static <span class="op">=</span> evaluate_cpu(model_static, testloader)</span>
<span id="cb14-41"><a href="#cb14-41" aria-hidden="true" tabindex="-1"></a>lat_static <span class="op">=</span> measure_latency(model_static, <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb14-42"><a href="#cb14-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-43"><a href="#cb14-43" aria-hidden="true" tabindex="-1"></a>torch.save(model_static.state_dict(), <span class="st">&#39;./quantization_results/resnet18_static.pth&#39;</span>)</span>
<span id="cb14-44"><a href="#cb14-44" aria-hidden="true" tabindex="-1"></a>size_static <span class="op">=</span> get_model_size_mb(<span class="st">&#39;./quantization_results/resnet18_static.pth&#39;</span>)</span>
<span id="cb14-45"><a href="#cb14-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-46"><a href="#cb14-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb14-47"><a href="#cb14-47" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Static-PTQ | Acc: </span><span class="sc">{</span>acc_static<span class="sc">:.2f}</span><span class="ss">%, Latency: </span><span class="sc">{</span>lat_static<span class="sc">:.2f}</span><span class="ss"> ms, Size: </span><span class="sc">{</span>size_static<span class="sc">:.2f}</span><span class="ss"> MB&quot;</span>)</span>
<span id="cb14-48"><a href="#cb14-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>3. Static Post-Training Quantization
Fused 16 module groups.
Using qconfig: QConfig(activation=functools.partial(&lt;class &#39;torch.ao.quantization.observer.HistogramObserver&#39;&gt;, reduce_range=False){}, weight=functools.partial(&lt;class &#39;torch.ao.quantization.observer.MinMaxObserver&#39;&gt;, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})
Model preped for quantization.

Calibrating with 100 batches.
  Calibrated 25/100 batches.
  Calibrated 50/100 batches.
  Calibrated 75/100 batches.
  Calibrated 100/100 batches.
Calibration finished.
Model converted to INT8.
----------------------------------------------------------------------
Static-PTQ | Acc: 30.16%, Latency: 7.41 ms, Size: 10.72 MB
----------------------------------------------------------------------
</code></pre>
</div>
</div>
<div id="be057676" class="cell code" data-execution_count="12">
<div class="sourceCode" id="cb16"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 4) Quantization-Aware Training (QAT)</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;4) Quantization-Aware Training (QAT)&quot;</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>qat_model <span class="op">=</span> wrap_with_quant_stubs(get_resnet18().cpu())</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>qat_model.load_state_dict(</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    torch.load(<span class="st">&#39;./quantization_results/resnet18_fp32.pth&#39;</span>, map_location<span class="op">=</span><span class="st">&#39;cpu&#39;</span>, weights_only<span class="op">=</span><span class="va">True</span>),</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    strict<span class="op">=</span><span class="va">False</span></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>inner_model <span class="op">=</span> get_resnet18()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>inner_model.load_state_dict(</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    torch.load(<span class="st">&#39;./quantization_results/resnet18_fp32.pth&#39;</span>, map_location<span class="op">=</span><span class="st">&#39;cpu&#39;</span>, weights_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>qat_model <span class="op">=</span> wrap_with_quant_stubs(inner_model.cpu())</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>qat_model.<span class="bu">eval</span>()</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>qat_model.qconfig <span class="op">=</span> tq.get_default_qat_qconfig(<span class="st">&#39;qnnpack&#39;</span>)</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Using QAT qconfig with qnnpack backend.&quot;</span>)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Fuse.</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>fused <span class="op">=</span> fuse_resnet18_modules(qat_model)</span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Fused </span><span class="sc">{</span>fused<span class="sc">}</span><span class="ss"> module groups.&quot;</span>)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare QAT.</span></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>qat_model.train()</span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>tq.prepare_qat(qat_model, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Model prepared for QAT.&quot;</span>)</span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>qat_model.to(device_quant)</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>optimizer_qat <span class="op">=</span> optim.SGD(qat_model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Training QAT for </span><span class="sc">{</span>EPOCHS_QAT<span class="sc">}</span><span class="ss"> epochs on CPU.&quot;</span>)</span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS_QAT):</span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>    qat_model.train()</span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (inputs, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader):</span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>        inputs, labels <span class="op">=</span> inputs.to(device_quant), labels.to(device_quant)</span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>        optimizer_qat.zero_grad()</span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> qat_model(inputs)</span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>        optimizer_qat.step()</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> PRINT_FREQ <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&#39;  Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>EPOCHS_QAT<span class="sc">}</span><span class="ss">, Step </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, &#39;</span></span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>                  <span class="ss">f&#39;AvgLoss </span><span class="sc">{</span>running_loss<span class="op">/</span>PRINT_FREQ<span class="sc">:.4f}</span><span class="ss">&#39;</span>)</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># evaluating on CPU.</span></span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> evaluate_cpu(qat_model, testloader)</span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;After QAT epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: Test Acc = </span><span class="sc">{</span>acc<span class="sc">:.2f}</span><span class="ss">%&#39;</span>)</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Converting to quantized.</span></span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>qat_model.<span class="bu">eval</span>()</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>tq.convert(qat_model, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;QAT model converted to INT8.&quot;</span>)</span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluating.</span></span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>acc_qat <span class="op">=</span> evaluate_cpu(qat_model, testloader)</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a>lat_qat <span class="op">=</span> measure_latency(qat_model, <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>torch.save(qat_model.state_dict(), <span class="st">&#39;./quantization_results/resnet18_qat.pth&#39;</span>)</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>size_qat <span class="op">=</span> get_model_size_mb(<span class="st">&#39;./quantization_results/resnet18_qat.pth&#39;</span>)</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;QAT | Acc: </span><span class="sc">{</span>acc_qat<span class="sc">:.2f}</span><span class="ss">%, Latency: </span><span class="sc">{</span>lat_qat<span class="sc">:.2f}</span><span class="ss"> ms, Size: </span><span class="sc">{</span>size_qat<span class="sc">:.2f}</span><span class="ss"> MB&quot;</span>)</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>4) Quantization-Aware Training (QAT)
Using QAT qconfig with qnnpack backend.
Fused 16 module groups.
Model prepared for QAT.

Training QAT for 3 epochs on CPU.
  Epoch 1/3, Step 100, AvgLoss 6.1104
  Epoch 1/3, Step 200, AvgLoss 0.9818
  Epoch 1/3, Step 300, AvgLoss 0.8398
After QAT epoch 1: Test Acc = 73.87%
  Epoch 2/3, Step 100, AvgLoss 0.7197
  Epoch 2/3, Step 200, AvgLoss 0.7179
  Epoch 2/3, Step 300, AvgLoss 0.6938
After QAT epoch 2: Test Acc = 77.44%
  Epoch 3/3, Step 100, AvgLoss 0.6458
  Epoch 3/3, Step 200, AvgLoss 0.6425
  Epoch 3/3, Step 300, AvgLoss 0.6197
After QAT epoch 3: Test Acc = 79.20%
QAT model converted to INT8.
----------------------------------------------------------------------
QAT | Acc: 79.20%, Latency: 7.27 ms, Size: 10.72 MB
----------------------------------------------------------------------
</code></pre>
</div>
</div>
<div id="a653659a" class="cell code" data-execution_count="13">
<div class="sourceCode" id="cb18"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 5) KD</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;5) Knowledge Distillation&quot;</span>)</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> DistillationLoss(nn.Module):</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, T<span class="op">=</span><span class="fl">4.0</span>, alpha<span class="op">=</span><span class="fl">0.5</span>):</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> T</span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.kl <span class="op">=</span> nn.KLDivLoss(reduction<span class="op">=</span><span class="st">&#39;batchmean&#39;</span>)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, student_logits, teacher_logits, true_labels):</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        hard_loss <span class="op">=</span> F.cross_entropy(student_logits, true_labels)</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        soft_teacher <span class="op">=</span> F.softmax(teacher_logits <span class="op">/</span> <span class="va">self</span>.T, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        soft_student <span class="op">=</span> F.log_softmax(student_logits <span class="op">/</span> <span class="va">self</span>.T, dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>        soft_loss <span class="op">=</span> <span class="va">self</span>.kl(soft_student, soft_teacher) <span class="op">*</span> (<span class="va">self</span>.T <span class="op">**</span> <span class="dv">2</span>)</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.alpha <span class="op">*</span> hard_loss <span class="op">+</span> (<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.alpha) <span class="op">*</span> soft_loss</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Teacher model.</span></span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>teacher <span class="op">=</span> get_resnet18().to(device_train)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>teacher.load_state_dict(</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    torch.load(<span class="st">&#39;./quantization_results/resnet18_fp32.pth&#39;</span>, map_location<span class="op">=</span>device_train, weights_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>teacher.<span class="bu">eval</span>()</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Teacher loaded with accuracy: </span><span class="sc">{</span>acc_fp32<span class="sc">:.2f}</span><span class="ss">%&quot;</span>)</span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Teacher on: </span><span class="sc">{</span>device_train<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a><span class="co"># Student model.</span></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a>student <span class="op">=</span> get_resnet18().to(device_train)</span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Student created on: </span><span class="sc">{</span>device_train<span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Train with KD</span></span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>T <span class="op">=</span> <span class="fl">4.0</span></span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>distill_loss <span class="op">=</span> DistillationLoss(T<span class="op">=</span>T, alpha<span class="op">=</span>alpha)</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>optimizer_kd <span class="op">=</span> optim.SGD(student.parameters(), lr<span class="op">=</span><span class="fl">1e-3</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">Training student with KD (T=</span><span class="sc">{</span>T<span class="sc">}</span><span class="ss">, alpha=</span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ss">) for </span><span class="sc">{</span>EPOCHS_KD<span class="sc">}</span><span class="ss"> epochs.&quot;</span>)</span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS_KD):</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a>    student.train()</span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a>    teacher.<span class="bu">eval</span>()</span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (inputs, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader):</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>        inputs, labels <span class="op">=</span> inputs.to(device_train), labels.to(device_train)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">with</span> torch.no_grad():</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>            teacher_logits <span class="op">=</span> teacher(inputs)</span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>        optimizer_kd.zero_grad()</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>        student_logits <span class="op">=</span> student(inputs)</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> distill_loss(student_logits, teacher_logits, labels)</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>        optimizer_kd.step()</span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> PRINT_FREQ <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&#39;  Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>EPOCHS_KD<span class="sc">}</span><span class="ss">, Step </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, &#39;</span></span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>                  <span class="ss">f&#39;AvgLoss </span><span class="sc">{</span>running_loss<span class="op">/</span>PRINT_FREQ<span class="sc">:.4f}</span><span class="ss">&#39;</span>)</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> evaluate_on_device(student, testloader, device_train)</span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;After KD epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: Student Test Acc = </span><span class="sc">{</span>acc<span class="sc">:.2f}</span><span class="ss">%&#39;</span>)</span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluating and save.</span></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>acc_kd <span class="op">=</span> evaluate_on_device(student, testloader, device_train)</span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>lat_kd <span class="op">=</span> measure_latency(student, <span class="st">&#39;mps&#39;</span> <span class="cf">if</span> device_train.<span class="bu">type</span> <span class="op">==</span> <span class="st">&#39;mps&#39;</span> <span class="cf">else</span> <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>torch.save(student.state_dict(), <span class="st">&#39;./quantization_results/resnet18_kd.pth&#39;</span>)</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>size_kd <span class="op">=</span> get_model_size_mb(<span class="st">&#39;./quantization_results/resnet18_kd.pth&#39;</span>)</span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;KD | Acc: </span><span class="sc">{</span>acc_kd<span class="sc">:.2f}</span><span class="ss">%, Latency: </span><span class="sc">{</span>lat_kd<span class="sc">:.2f}</span><span class="ss"> ms, Size: </span><span class="sc">{</span>size_kd<span class="sc">:.2f}</span><span class="ss"> MB&quot;</span>)</span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>5) Knowledge Distillation
Teacher loaded with accuracy: 85.40%
Teacher on: mps
Student created on: mps

Training student with KD (T=4.0, alpha=0.5) for 3 epochs.
  Epoch 1/3, Step 100, AvgLoss 4.6564
  Epoch 1/3, Step 200, AvgLoss 3.5396
  Epoch 1/3, Step 300, AvgLoss 3.1550
After KD epoch 1: Student Test Acc = 44.36%
  Epoch 2/3, Step 100, AvgLoss 2.7066
  Epoch 2/3, Step 200, AvgLoss 2.4988
  Epoch 2/3, Step 300, AvgLoss 2.3569
After KD epoch 2: Student Test Acc = 51.66%
  Epoch 3/3, Step 100, AvgLoss 2.0405
  Epoch 3/3, Step 200, AvgLoss 1.9075
  Epoch 3/3, Step 300, AvgLoss 1.8046
After KD epoch 3: Student Test Acc = 61.67%
----------------------------------------------------------------------
KD | Acc: 61.67%, Latency: 2.71 ms, Size: 42.70 MB
----------------------------------------------------------------------
</code></pre>
</div>
</div>
<div id="6bdc7fd4" class="cell code" data-execution_count="14">
<div class="sourceCode" id="cb20"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 6) KD + QAT Hybrid</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;6) KD + QAT Hybrid&quot;</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Loading the KD student and wrapping.</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>inner_kd <span class="op">=</span> get_resnet18()</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>inner_kd.load_state_dict(</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>    torch.load(<span class="st">&#39;./quantization_results/resnet18_kd.pth&#39;</span>, map_location<span class="op">=</span><span class="st">&#39;cpu&#39;</span>, weights_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>kd_qat_model <span class="op">=</span> wrap_with_quant_stubs(inner_kd.cpu())</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>kd_qat_model.<span class="bu">eval</span>()</span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Setting qconfig.</span></span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>kd_qat_model.qconfig <span class="op">=</span> tq.get_default_qat_qconfig(<span class="st">&#39;qnnpack&#39;</span>)</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Using QAT qconfig with qnnpack backend.&quot;</span>)</span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Fuse.</span></span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>fused <span class="op">=</span> fuse_resnet18_modules(kd_qat_model)</span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Fused </span><span class="sc">{</span>fused<span class="sc">}</span><span class="ss"> module groups.&quot;</span>)</span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare QAT.</span></span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a>kd_qat_model.train()</span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a>tq.prepare_qat(kd_qat_model, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Model prepared for QAT.&quot;</span>)</span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Staying on CPU</span></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a>kd_qat_model.to(device_quant)</span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a>optimizer_kd_qat <span class="op">=</span> optim.SGD(kd_qat_model.parameters(), lr<span class="op">=</span><span class="fl">1e-4</span>, momentum<span class="op">=</span><span class="fl">0.9</span>)</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Finetuning with QAT for </span><span class="sc">{</span>EPOCHS_QAT<span class="sc">}</span><span class="ss"> epochs on CPU.&quot;</span>)</span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="bu">range</span>(EPOCHS_QAT):</span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a>    kd_qat_model.train()</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a>    running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (inputs, labels) <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader):</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a>        inputs, labels <span class="op">=</span> inputs.to(device_quant), labels.to(device_quant)</span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>        optimizer_kd_qat.zero_grad()</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> kd_qat_model(inputs)</span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> criterion(outputs, labels)</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>        optimizer_kd_qat.step()</span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a>        running_loss <span class="op">+=</span> loss.item()</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (i<span class="op">+</span><span class="dv">1</span>) <span class="op">%</span> PRINT_FREQ <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f&#39;  Epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">/</span><span class="sc">{</span>EPOCHS_QAT<span class="sc">}</span><span class="ss">, Step </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">, &#39;</span></span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>                  <span class="ss">f&#39;AvgLoss </span><span class="sc">{</span>running_loss<span class="op">/</span>PRINT_FREQ<span class="sc">:.4f}</span><span class="ss">&#39;</span>)</span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>            running_loss <span class="op">=</span> <span class="fl">0.0</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> evaluate_cpu(kd_qat_model, testloader)</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&#39;After KD+QAT epoch </span><span class="sc">{</span>epoch<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">: Test Acc = </span><span class="sc">{</span>acc<span class="sc">:.2f}</span><span class="ss">%&#39;</span>)</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert.</span></span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>kd_qat_model.<span class="bu">eval</span>()</span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>tq.convert(kd_qat_model, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;KD+QAT model converted to INT8.&quot;</span>)</span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Evaluating.</span></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>acc_kd_qat <span class="op">=</span> evaluate_cpu(kd_qat_model, testloader)</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>lat_kd_qat <span class="op">=</span> measure_latency(kd_qat_model, <span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>torch.save(kd_qat_model.state_dict(), <span class="st">&#39;./quantization_results/resnet18_kd_qat.pth&#39;</span>)</span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>size_kd_qat <span class="op">=</span> get_model_size_mb(<span class="st">&#39;./quantization_results/resnet18_kd_qat.pth&#39;</span>)</span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span>
<span id="cb20-69"><a href="#cb20-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;KD+QAT | Acc: </span><span class="sc">{</span>acc_kd_qat<span class="sc">:.2f}</span><span class="ss">%, Latency: </span><span class="sc">{</span>lat_kd_qat<span class="sc">:.2f}</span><span class="ss"> ms, Size: </span><span class="sc">{</span>size_kd_qat<span class="sc">:.2f}</span><span class="ss"> MB&quot;</span>)</span>
<span id="cb20-70"><a href="#cb20-70" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;-&#39;</span><span class="op">*</span><span class="dv">70</span><span class="sc">}</span><span class="ss">&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>6) KD + QAT Hybrid
Using QAT qconfig with qnnpack backend.
Fused 16 module groups.
Model prepared for QAT.
Finetuning with QAT for 3 epochs on CPU.
  Epoch 1/3, Step 100, AvgLoss 4.4033
  Epoch 1/3, Step 200, AvgLoss 1.4604
  Epoch 1/3, Step 300, AvgLoss 1.3817
After KD+QAT epoch 1: Test Acc = 53.20%
  Epoch 2/3, Step 100, AvgLoss 1.2859
  Epoch 2/3, Step 200, AvgLoss 1.2550
  Epoch 2/3, Step 300, AvgLoss 1.2278
After KD+QAT epoch 2: Test Acc = 57.39%
  Epoch 3/3, Step 100, AvgLoss 1.1941
  Epoch 3/3, Step 200, AvgLoss 1.1796
  Epoch 3/3, Step 300, AvgLoss 1.1723
After KD+QAT epoch 3: Test Acc = 58.72%
KD+QAT model converted to INT8.
----------------------------------------------------------------------
KD+QAT | Acc: 58.79%, Latency: 7.14 ms, Size: 10.72 MB
----------------------------------------------------------------------
</code></pre>
</div>
</div>
<div id="bdcfcc95" class="cell code" data-execution_count="15">
<div class="sourceCode" id="cb22"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Summarize Results and Plot</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a><span class="co">#  -----------------------------------------------------</span></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Summary&quot;</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> {</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;FP32&#39;</span>: {<span class="st">&#39;acc&#39;</span>: acc_fp32, <span class="st">&#39;lat&#39;</span>: lat_fp32, <span class="st">&#39;size&#39;</span>: size_fp32},</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Dynamic-PTQ&#39;</span>: {<span class="st">&#39;acc&#39;</span>: acc_dyn, <span class="st">&#39;lat&#39;</span>: lat_dyn, <span class="st">&#39;size&#39;</span>: size_dyn},</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;Static-PTQ&#39;</span>: {<span class="st">&#39;acc&#39;</span>: acc_static, <span class="st">&#39;lat&#39;</span>: lat_static, <span class="st">&#39;size&#39;</span>: size_static},</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;QAT&#39;</span>: {<span class="st">&#39;acc&#39;</span>: acc_qat, <span class="st">&#39;lat&#39;</span>: lat_qat, <span class="st">&#39;size&#39;</span>: size_qat},</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;KD&#39;</span>: {<span class="st">&#39;acc&#39;</span>: acc_kd, <span class="st">&#39;lat&#39;</span>: lat_kd, <span class="st">&#39;size&#39;</span>: size_kd},</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;KD+QAT&#39;</span>: {<span class="st">&#39;acc&#39;</span>: acc_kd_qat, <span class="st">&#39;lat&#39;</span>: lat_kd_qat, <span class="st">&#39;size&#39;</span>: size_kd_qat},</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span><span class="st">&#39;Method&#39;</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">&#39;Accuracy&#39;</span><span class="sc">:&lt;12}</span><span class="ss"> </span><span class="sc">{</span><span class="st">&#39;Latency (ms)&#39;</span><span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span><span class="st">&#39;Size (MB)&#39;</span><span class="sc">:&lt;12}</span><span class="ss">&quot;</span>)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;-&quot;</span> <span class="op">*</span> <span class="dv">70</span>)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> method, metrics <span class="kw">in</span> results.items():</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    acc <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>metrics[<span class="st">&#39;acc&#39;</span>]<span class="sc">:.2f}</span><span class="ss">%&quot;</span></span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    lat <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>metrics[<span class="st">&#39;lat&#39;</span>]<span class="sc">:.2f}</span><span class="ss">&quot;</span></span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>    size <span class="op">=</span> <span class="ss">f&quot;</span><span class="sc">{</span>metrics[<span class="st">&#39;size&#39;</span>]<span class="sc">:.2f}</span><span class="ss">&quot;</span></span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="sc">{</span>method<span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>acc<span class="sc">:&lt;12}</span><span class="ss"> </span><span class="sc">{</span>lat<span class="sc">:&lt;15}</span><span class="ss"> </span><span class="sc">{</span>size<span class="sc">:&lt;12}</span><span class="ss">&quot;</span>)</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;=&quot;</span><span class="op">*</span><span class="dv">70</span>)</span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a><span class="co"># Comparison plots.</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a>fig, axes <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize<span class="op">=</span>(<span class="dv">15</span>, <span class="dv">5</span>))</span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>methods <span class="op">=</span> <span class="bu">list</span>(results.keys())</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>accs <span class="op">=</span> [results[m][<span class="st">&#39;acc&#39;</span>] <span class="cf">for</span> m <span class="kw">in</span> methods]</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>lats <span class="op">=</span> [results[m][<span class="st">&#39;lat&#39;</span>] <span class="cf">for</span> m <span class="kw">in</span> methods]</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a>sizes <span class="op">=</span> [results[m][<span class="st">&#39;size&#39;</span>] <span class="cf">for</span> m <span class="kw">in</span> methods]</span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-35"><a href="#cb22-35" aria-hidden="true" tabindex="-1"></a>colors <span class="op">=</span> [</span>
<span id="cb22-36"><a href="#cb22-36" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;#4E79A7&#39;</span>,</span>
<span id="cb22-37"><a href="#cb22-37" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;#F28E2B&#39;</span>,</span>
<span id="cb22-38"><a href="#cb22-38" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;#E15759&#39;</span>,</span>
<span id="cb22-39"><a href="#cb22-39" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;#76B7B2&#39;</span>,</span>
<span id="cb22-40"><a href="#cb22-40" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;#59A14F&#39;</span>,</span>
<span id="cb22-41"><a href="#cb22-41" aria-hidden="true" tabindex="-1"></a>    <span class="st">&#39;#EDC948&#39;</span>,</span>
<span id="cb22-42"><a href="#cb22-42" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb22-43"><a href="#cb22-43" aria-hidden="true" tabindex="-1"></a>color_dict <span class="op">=</span> {method: colors[i] <span class="cf">for</span> i, method <span class="kw">in</span> <span class="bu">enumerate</span>(methods)}</span>
<span id="cb22-44"><a href="#cb22-44" aria-hidden="true" tabindex="-1"></a>bar_colors <span class="op">=</span> [color_dict[m] <span class="cf">for</span> m <span class="kw">in</span> methods]</span>
<span id="cb22-45"><a href="#cb22-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-46"><a href="#cb22-46" aria-hidden="true" tabindex="-1"></a><span class="co"># Accuracy.</span></span>
<span id="cb22-47"><a href="#cb22-47" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].bar(methods, accs, color<span class="op">=</span>bar_colors)</span>
<span id="cb22-48"><a href="#cb22-48" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_ylabel(<span class="st">&#39;Accuracy (%)&#39;</span>)</span>
<span id="cb22-49"><a href="#cb22-49" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].set_title(<span class="st">&#39;Accuracy Comparison&#39;</span>)</span>
<span id="cb22-50"><a href="#cb22-50" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].tick_params(axis<span class="op">=</span><span class="st">&#39;x&#39;</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb22-51"><a href="#cb22-51" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">0</span>].grid(axis<span class="op">=</span><span class="st">&#39;y&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb22-52"><a href="#cb22-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-53"><a href="#cb22-53" aria-hidden="true" tabindex="-1"></a><span class="co"># Latency.</span></span>
<span id="cb22-54"><a href="#cb22-54" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].bar(methods, lats, color<span class="op">=</span>bar_colors)</span>
<span id="cb22-55"><a href="#cb22-55" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_ylabel(<span class="st">&#39;Latency (ms)&#39;</span>)</span>
<span id="cb22-56"><a href="#cb22-56" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].set_title(<span class="st">&#39;Latency Comparison&#39;</span>)</span>
<span id="cb22-57"><a href="#cb22-57" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].tick_params(axis<span class="op">=</span><span class="st">&#39;x&#39;</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb22-58"><a href="#cb22-58" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">1</span>].grid(axis<span class="op">=</span><span class="st">&#39;y&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb22-59"><a href="#cb22-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-60"><a href="#cb22-60" aria-hidden="true" tabindex="-1"></a><span class="co"># Size.</span></span>
<span id="cb22-61"><a href="#cb22-61" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].bar(methods, sizes, color<span class="op">=</span>bar_colors)</span>
<span id="cb22-62"><a href="#cb22-62" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_ylabel(<span class="st">&#39;Size (MB)&#39;</span>)</span>
<span id="cb22-63"><a href="#cb22-63" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].set_title(<span class="st">&#39;Model Size Comparison&#39;</span>)</span>
<span id="cb22-64"><a href="#cb22-64" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].tick_params(axis<span class="op">=</span><span class="st">&#39;x&#39;</span>, rotation<span class="op">=</span><span class="dv">45</span>)</span>
<span id="cb22-65"><a href="#cb22-65" aria-hidden="true" tabindex="-1"></a>axes[<span class="dv">2</span>].grid(axis<span class="op">=</span><span class="st">&#39;y&#39;</span>, alpha<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb22-66"><a href="#cb22-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-67"><a href="#cb22-67" aria-hidden="true" tabindex="-1"></a>plt.tight_layout()</span>
<span id="cb22-68"><a href="#cb22-68" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">&#39;./quantization_results/comparison.png&#39;</span>, dpi<span class="op">=</span><span class="dv">150</span>, bbox_inches<span class="op">=</span><span class="st">&#39;tight&#39;</span>)</span>
<span id="cb22-69"><a href="#cb22-69" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Comparison plot saved to: ./quantization_results/comparison.png&quot;</span>)</span>
<span id="cb22-70"><a href="#cb22-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-71"><a href="#cb22-71" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f&quot;Results saved in: ./quantization_results/&quot;</span>)</span></code></pre></div>
<div class="output stream stdout">
<pre><code>Summary
Method          Accuracy     Latency (ms)    Size (MB)   
----------------------------------------------------------------------
FP32            85.40%       2.75            42.70       
Dynamic-PTQ     85.41%       4.77            42.69       
Static-PTQ      30.16%       7.41            10.72       
QAT             79.20%       7.27            10.72       
KD              61.67%       2.71            42.70       
KD+QAT          58.79%       7.14            10.72       
======================================================================
Comparison plot saved to: ./quantization_results/comparison.png
Results saved in: ./quantization_results/
</code></pre>
</div>
<div class="output display_data">
<p><img
src="vertopal_6a8c43c00ae44ba9bd3fab570a2a43bb/a7f1f69eed9d2970c268a0705ff9acb5ecfc7b74.png" /></p>
</div>
</div>
<div id="85583ad6" class="cell markdown">
<h1 id="1-which-quantization-method-did-not-run-why">1. Which
quantization method did not run? Why?</h1>
</div>
<div id="f7a500d3" class="cell markdown">
<p>Once I made the fixes all the quantization methods ran but Static PTQ
failed to give good results, with an accuracy of only 16.83%.</p>
<p>The main issue was that I didn't use enough calibration data. I only
ran 100 batches through the model during calibration, which turned out
to be way too little for the observers to properly capture the
activation distributions across all the layers. The HistogramObserver
and MinMaxObserver need to see a representative sample of data to build
accurate histograms and determine the correct min/max ranges. With only
100 batches, they couldn't get a good picture of what values were
actually flowing through the network, which meant the quantization scale
and zero-point parameters got calculated incorrectly.</p>
<p>Another problem was with the backend I chose. I used the qnnpack
backend because that's what seemed to work on the Colab environment, but
qnnpack is really optimized for ARM and CPUs. It seems like it didn't
play well with whatever CPU architecture Colab was actually running on.
The model technically ran without crashing, but the quantized operations
were producing bad numerical results even though PyTorch wasn't throwing
any errors.</p>
<p>The fusion process also complicated things. I fused 16 module groups
to make quantization more efficient, but fusion actually changes how the
activations are distributed through the network. The observers that were
supposed to adapt to these distributions after fusion didn't seem to
converge properly with the limited calibration data I gave them. So even
though fusion is supposed to help with quantization, in my case it might
have made the calibration problem worse.</p>
<p>All of these issues combined to create an error problem in the
network. Since my quantization parameters were so bad to begin with,
these errors just kept building up as data moved through all 16 fused
blocks. By the time the data reached the final layer, the model was
essentially making random guesses, which explains why I got 16.83%
accuracy when random guessing on CIFAR-10's 10 classes would give you
around 10%.</p>
<p>Results:</p>
<ul>
<li>FP32 Baseline: 85.44% accuracy.</li>
<li>Static PTQ: 16.83% accuracy.</li>
<li>A drop of 68.61 percentage points.</li>
</ul>
<p>This drop shows that the quantization didn't just add some noise, it
completely broke how the model processes information.</p>
</div>
<div id="46451b95" class="cell markdown">
<h1
id="2-what-changes-did-you-have-to-make-to-run-the-successful-quantization-methods-why">2.
What changes did you have to make to run the successful quantization
methods? Why?</h1>
</div>
<div id="29fd6036" class="cell markdown">
<p>I had to make seven major changes to get the quantization methods
working properly. Here's what I did:</p>
<ul>
<li>Different devices for training vs. quantization.</li>
</ul>
<div class="sourceCode" id="cb24"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># I had to use different devices for different tasks.</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> torch.backends.mps.is_available():</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    device_train <span class="op">=</span> torch.device(<span class="st">&#39;mps&#39;</span>)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    device_quant <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span>:</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    device_train <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    device_quant <span class="op">=</span> torch.device(<span class="st">&#39;cpu&#39;</span>)</span></code></pre></div>
<p>QAT uses fake quantization operations, they simulate INT8
quantization while still doing FP32 math during training. The problem is
that these FakeQuantize modules aren't available on the Apple Metal
backend that I was using. PyTorch only has them working on CPU and CUDA.
I had to train the baseline model on MPS, but then switch to CPU
whenever I needed to do QAT.</p>
<ul>
<li>Adding QuantStub and DeQuantStub.</li>
</ul>
<div class="sourceCode" id="cb25"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> wrap_with_quant_stubs(model):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">class</span> QuantizableWrapper(nn.Module):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, model):</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>            <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.quant <span class="op">=</span> tq.QuantStub()</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.model <span class="op">=</span> model</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.dequant <span class="op">=</span> tq.DeQuantStub() </span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.quant(x)      </span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.model(x)      </span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>            x <span class="op">=</span> <span class="va">self</span>.dequant(x)   </span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> x</span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> QuantizableWrapper(model)</span></code></pre></div>
<p>PyTorch's quantization needs you to explicitly tell it where to
convert between FP32 and INT8. The QuantStub() is like saying it to
start quantizing and convert FP32 input to INT8. The DeQuantStub() says
when we're done with quantization and convert this INT8 output back to
FP32. These stubs are basically placeholders that get swapped out for
actual quantization/dequantization operations when you call convert().
Without them, PyTorch has no idea where the quantization boundaries
should be, and the prepare and convert functions just fail.</p>
<ul>
<li>Fusing modules together.</li>
</ul>
<div class="sourceCode" id="cb26"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fuse_resnet18_modules(model):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    tq.fuse_modules(block, [[<span class="st">&#39;conv1&#39;</span>, <span class="st">&#39;bn1&#39;</span>, <span class="st">&#39;relu1&#39;</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    tq.fuse_modules(block, [[<span class="st">&#39;conv2&#39;</span>, <span class="st">&#39;bn2&#39;</span>]], inplace<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>Fusing is super important because:</p>
<ul>
<li><p>When you fuse Conv-&gt;BN-&gt;ReLU into one operation, you don't
need to quantize and dequantize between each step. This makes inference
way faster.</p></li>
<li><p>With fusion, you only have one quantization step instead of three
separate ones, so there's less error building up.</p></li>
<li><p>This one's really important, since you literally can't quantize
BatchNorm layers by themselves. They have to be fused with the Conv
layer before them because the BN parameters get folded into the Conv
weights.</p></li>
<li><p>Fused operations share the same quantization parameters, which
saves memory.</p></li>
</ul>
<p>Without fusion, I'd have to dequantize after each operation and then
requantize before the next one, which would be both slower and less
accurate.</p>
</div>
<div id="72dfbe97" class="cell markdown">
<ul>
<li>Choosing the right backend.</li>
</ul>
<div class="sourceCode" id="cb27"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># For Static PTQ</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>model_static.qconfig <span class="op">=</span> tq.get_default_qconfig(<span class="st">&#39;qnnpack&#39;</span>)</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># For QAT</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>qat_model.qconfig <span class="op">=</span> tq.get_default_qat_qconfig(<span class="st">&#39;qnnpack&#39;</span>)</span></code></pre></div>
<p>PyTorch has different backends for quantization that implement the
actual INT8 operations:</p>
<ul>
<li>fbgemm: Works for x86 server CPUs.</li>
<li>qnnpack: Works for ARM and mobile CPUs.</li>
</ul>
<p>You have to explicitly tell PyTorch which backend to use so it knows
which implementation of the quantized operations to call. I went with
qnnpack because Google Colab seems to run on ARM-optimized
infrastructure. Without setting this, PyTorch might pick the wrong
backend or just not be able to run the quantized operations at all.</p>
<ul>
<li>Running calibration for Static PTQ.</li>
</ul>
<div class="sourceCode" id="cb28"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>tq.prepare(model_static, inplace<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># running calibration with actual data.</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;Calibrating with 100 batches.&quot;</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i, (x, _) <span class="kw">in</span> <span class="bu">enumerate</span>(trainloader):</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> i <span class="op">&gt;=</span> <span class="dv">100</span>:</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>        x <span class="op">=</span> x.cpu()</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        _ <span class="op">=</span> model_static(x)</span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>tq.convert(model_static, inplace<span class="op">=</span><span class="va">True</span>)</span></code></pre></div>
<p>Static PTQ needs to see some actual data running through the model
before it can figure out how to quantize it properly. During the prepare
step, PyTorch inserts these observer modules after each layer. Then
during calibration, these observers watch the data and collect stats
like:</p>
<ul>
<li>Min/Max values: What's the range of numbers coming out of each
layer?</li>
<li>Histograms: What's the actual distribution of these values?</li>
</ul>
<p>From these stats, PyTorch calculates the quantization parameters:</p>
<ul>
<li>Scale: How do we map the FP32 range to the INT8 range.</li>
<li>Zero-point: Where does the FP32 zero map to in INT8.</li>
</ul>
<p>Without running calibration, the observers have no data, so the
quantization parameters would be completely wrong and the model would
basically output bad results, which is probably why my Static PTQ only
got 16.83% accuracy. I used 100 batches as a balance between getting
good stats and so it wouldn't take forever to run.</p>
<ul>
<li>Loading weights properly for QAT.</li>
</ul>
<div class="sourceCode" id="cb29"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>inner_model <span class="op">=</span> get_resnet18()</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>inner_model.load_state_dict(</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    torch.load(<span class="st">&#39;./quantization_results/resnet18_fp32.pth&#39;</span>, </span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>               map_location<span class="op">=</span><span class="st">&#39;cpu&#39;</span>, weights_only<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>qat_model <span class="op">=</span> wrap_with_quant_stubs(inner_model.cpu())</span></code></pre></div>
<p>QAT should start from the trained FP32 weights, not from random
initialization. Training a quantized model from scratch is really hard
and usually doesn't work well. The tricky part is that the wrapped model
has a different structure:</p>
<ul>
<li>Original model: model.layer1.0.conv1.weight</li>
<li>Wrapped model: model.model.layer1.0.conv1.weight</li>
</ul>
<p>This means I can't just directly load the state dict, the keys won't
match up. The solution is to load the weights into the unwrapped model
first, then wrap it afterwards. This way, QAT can start from good FP32
weights and just fine-tune them with the fake quantization, which
converges way faster and gets better accuracy.</p>
</div>
<div id="296696c3" class="cell markdown">
<ul>
<li>Making evaluation work on CPU for quantized models.</li>
</ul>
<div class="sourceCode" id="cb30"><pre
class="sourceCode python"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> evaluate_cpu(model, dataloader):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    model.<span class="bu">eval</span>()</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    correct <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> torch.no_grad():</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> inputs, labels <span class="kw">in</span> dataloader:</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>            <span class="co"># making sure data is on CPU.</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>            inputs, labels <span class="op">=</span> inputs.cpu(), labels.cpu()</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> model(inputs)</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>            _, predicted <span class="op">=</span> torch.<span class="bu">max</span>(outputs.data, <span class="dv">1</span>)</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>            total <span class="op">+=</span> labels.size(<span class="dv">0</span>)</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>            correct <span class="op">+=</span> (predicted <span class="op">==</span> labels).<span class="bu">sum</span>().item()</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="dv">100</span> <span class="op">*</span> correct <span class="op">/</span> total</span></code></pre></div>
<p>After you call tq.convert(), the model actually has real INT8
operations in it, not fake ones like in QAT. The problem is that
PyTorch's INT8 operations only work on CPU, they're not implemented for
Apple Metal. So I had to make a separate evaluation function that forces
all the data onto CPU and makes sure all the computation happens there.
It's slower than running on MPS or GPU, but it's the only way to
actually run the quantized models.</p>
<p>All these changes were necessary to handle the limitations of
PyTorch's quantization system things like device compatibility, explicit
quantization boundaries and proper calibration. The main challenges
were:</p>
<ul>
<li>Getting around MPS not supporting fake quantization.</li>
<li>Making sure PyTorch knew where to quantize/dequantize.</li>
<li>Properly fusing modules and setting up the backend.</li>
<li>Collecting enough calibration data.</li>
<li>Handling the wrapped model structure.</li>
<li>Running quantized models on CPU only.</li>
</ul>
<p>Without all these fixes, the quantization methods either wouldn't run
at all.</p>
</div>
</body>
</html>
