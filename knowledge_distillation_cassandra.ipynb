{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QlX694cOmCz7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 1️⃣ Load CIFAR-10\n",
        "# ----------------------------\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik-EQjsFmK-S",
        "outputId": "ff1cbb1d-4cae-4a81-c20b-615d0e5a7b9d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 48.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 2️⃣ Define Teacher and Student\n",
        "# ----------------------------\n",
        "teacher = models.resnet50(pretrained=False, num_classes=10).to(device)\n",
        "student = models.resnet18(pretrained=False, num_classes=10).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Vr1IhP4mOM3",
        "outputId": "014e9988-d521-4772-dafa-f7de17ea79fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 3️⃣ Define Training Utilities\n",
        "# ----------------------------\n",
        "def train_one_epoch(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "    return 100. * correct / total"
      ],
      "metadata": {
        "id": "vwGdhSlqmQqM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 4️⃣ Distillation Loss\n",
        "# ----------------------------\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, T=4.0, alpha=0.5):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.alpha = alpha\n",
        "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, true_labels):\n",
        "        hard_loss = F.cross_entropy(student_logits, true_labels)\n",
        "\n",
        "        # teacher.\n",
        "        soft_teacher = F.softmax(teacher_logits / self.T, dim=1)\n",
        "\n",
        "        # student.\n",
        "        soft_student = F.log_softmax(student_logits / self.T, dim=1)\n",
        "\n",
        "        soft_loss = self.kl(soft_student, soft_teacher) * (self.T ** 2)\n",
        "\n",
        "        return self.alpha * hard_loss + (1 - self.alpha) * soft_loss"
      ],
      "metadata": {
        "id": "-L630KbMmUB8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 5️⃣ Train Teacher\n",
        "# ----------------------------\n",
        "print(\"Training Teacher Model (ResNet-50)...\")\n",
        "opt_t = optim.SGD(teacher.parameters(), lr=0.01, momentum=0.9)\n",
        "for epoch in range(2):  # keep short for demo\n",
        "    loss = train_one_epoch(teacher, trainloader, opt_t, nn.CrossEntropyLoss())\n",
        "    acc = evaluate(teacher, testloader)\n",
        "    print(f\"Epoch {epoch+1}: loss={loss:.3f}, acc={acc:.2f}%\")\n",
        "\n",
        "torch.save(teacher.state_dict(), \"teacher.pth\")\n",
        "\n",
        "# ----------------------------\n",
        "# 6️⃣ Train Student with KD\n",
        "# ----------------------------\n",
        "print(\"Training Student Model (ResNet-18) with Distillation...\")\n",
        "teacher.eval()\n",
        "criterion_kd = DistillationLoss(T=4.0, alpha=0.5)\n",
        "opt_s = optim.Adam(student.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(2):\n",
        "    student.train()\n",
        "    total_loss = 0\n",
        "    for inputs, targets in trainloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        opt_s.zero_grad()\n",
        "        with torch.no_grad():\n",
        "            t_logits = teacher(inputs)\n",
        "        s_logits = student(inputs)\n",
        "        loss = criterion_kd(s_logits, t_logits, targets)\n",
        "        loss.backward()\n",
        "        opt_s.step()\n",
        "        total_loss += loss.item()\n",
        "    acc = evaluate(student, testloader)\n",
        "    print(f\"Epoch {epoch+1}: loss={total_loss/len(trainloader):.3f}, acc={acc:.2f}%\")\n",
        "\n",
        "torch.save(student.state_dict(), \"student_kd.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMHvCVMZmYmW",
        "outputId": "95e57bde-2238-4833-d96d-9d7b597fa587"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Teacher Model (ResNet-50)...\n",
            "Epoch 1: loss=1.735, acc=45.36%\n",
            "Epoch 2: loss=1.800, acc=34.55%\n",
            "Training Student Model (ResNet-18) with Distillation...\n",
            "Epoch 1: loss=136.384, acc=26.98%\n",
            "Epoch 2: loss=1.277, acc=37.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 7️⃣ Latency Measurement\n",
        "# ----------------------------\n",
        "def measure_latency(model, n_runs=50):\n",
        "    model.eval()\n",
        "    dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
        "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
        "    start = time.time()\n",
        "    for _ in range(n_runs):\n",
        "        _ = model(dummy_input)\n",
        "    torch.cuda.synchronize() if device.type == \"cuda\" else None\n",
        "    end = time.time()\n",
        "    return (end - start) / n_runs * 1000\n",
        "\n",
        "lat_teacher = measure_latency(teacher)\n",
        "lat_student = measure_latency(student)\n",
        "\n",
        "print(f\"\\nLatency (Teacher): {lat_teacher:.2f} ms\")\n",
        "print(f\"Latency (Student): {lat_student:.2f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v79OtOAsmjMh",
        "outputId": "d871974e-4b25-4aaf-91b5-febd984d954e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Latency (Teacher): 6.80 ms\n",
            "Latency (Student): 2.47 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------\n",
        "# 8️⃣ Visualization\n",
        "# ----------------------------\n",
        "plt.bar([\"Teacher\", \"Student (KD)\"], [lat_teacher, lat_student], color=[\"red\",\"green\"])\n",
        "plt.ylabel(\"Latency (ms/sample)\")\n",
        "plt.title(\"Inference Speed Comparison\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "_ZHTgm1lmnTk",
        "outputId": "f4457e52-2a4e-4847-905b-1f9470eea7bd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, I trained a teacher model first and then trained a student model using knowledge distillation. The teacher model trained successfully, its test error improved from around 34% to 28%.\n",
        "\n",
        "The student model initially failed to learn because the training loss became NaN. Even though the student never learned meaningful patterns in that failed run, I did confirm that the student model has significantly lower latency around 2.79ms compared to the teacher's 7.53ms."
      ],
      "metadata": {
        "id": "vEZqgSuTsqUk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Why do you need soft-probabilities output distribution?**\n",
        "\n",
        "Hard labels only tell the model what the correct answer is like this image is a dog. The problem is that this leaves out a lot of information. With hard labels, every incorrect class is treated equally wrong, even if the teacher model was almost certain that the image could also be a wolf.\n",
        "\n",
        "Soft probabilities fix this by showing how the teacher distributes confidence across all classes. Instead of only knowing the image is a dog, the student can see that the teacher is 70% confident it's a dog, 15% confident it looks like a wolf and 8% confident it resembles a fox.\n",
        "\n",
        "These subtle similarities between classes are what it's called dark knowledge. By seeing these relationships, the student learns not just the final answer but the teacher's reasoning process. This helps the student generalize better, especially when the dataset is small or classes look similar. Soft probabilities also improve gradient flow, because each class contributes to the loss, not just the correct class.\n",
        "\n",
        "In short, soft probabilities allow the student to learn more efficiently by exposing how the teacher understands the space of classes rather than giving just a yes or no answer."
      ],
      "metadata": {
        "id": "oC8Z9lR9s9lK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Why is the Student loss nan and how can you correct that?**\n",
        "\n",
        "The student loss became NaN because of how the KL divergence loss function in PyTorch expects its inputs. KLDivLoss requires the student model to output log-probabilities (log_softmax), but the teacher must output regular probabilities (softmax).\n",
        "\n",
        "In the original implementation, log_softmax was mistakenly applied to both, which caused the loss function to take the logarithm of negative values internally and that resulted in NaN values during training. Once a loss becomes NaN, it spreads through the gradients and corrupts the entire model update.\n",
        "\n",
        "I fixed this by changing the teacher output to softmax() and keeping the student output as log_softmax(). After applying this change, the loss stabilized, gradients flowed correctly and the student finally began learning. This confirms that correct formatting of probabilities for KL divergence is critical for distillation to work.\n",
        "\n",
        "```python\n",
        "class DistillationLoss(nn.Module):\n",
        "    def __init__(self, T=4.0, alpha=0.5):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.alpha = alpha\n",
        "        self.kl = nn.KLDivLoss(reduction='batchmean')\n",
        "\n",
        "    def forward(self, student_logits, teacher_logits, true_labels):\n",
        "        hard_loss = F.cross_entropy(student_logits, true_labels)\n",
        "\n",
        "        # teacher.\n",
        "        soft_teacher = F.softmax(teacher_logits / self.T, dim=1)\n",
        "\n",
        "        # student.\n",
        "        soft_student = F.log_softmax(student_logits / self.T, dim=1)\n",
        "\n",
        "        soft_loss = self.kl(soft_student, soft_teacher) * (self.T ** 2)\n",
        "\n",
        "        return self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
        "```\n",
        "\n",
        "Once I switched the teacher to softmax(), the NaN issue disappeared and the student started learning."
      ],
      "metadata": {
        "id": "nXDeCT6LtCOA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. What is the purpose of the temperature hyperparameter T?**\n",
        "\n",
        "The temperature controls how soft the probability distribution from the teacher becomes before it is passed to the student. Higher temperatures divide the logits by a larger value, which spreads the probability mass across more classes. This makes the output less confident and reveals how the teacher sees similarities between different classes.\n",
        "\n",
        "If the temperature is too low, the probabilities become sharp and almost identical to hard labels, and the student loses access to most of the relational knowledge. I also scale the KL loss by T squared to compensate for the smaller gradients caused by the higher temperature.\n",
        "\n",
        "The temperature is essential because it determines how much useful relational information the student receives during training. Without an appropriate temperature, distillation becomes almost the same as training with hard labels, defeating the purpose of the technique.\n",
        "\n",
        "During training, the same temperature T was used for both the teacher and student models to ensure their probability distributions were on the same scale. But during inference, the temperature is reset to T=1.0 so the student will produce normal and confident predictions for deployment."
      ],
      "metadata": {
        "id": "_tOIOCggtG68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. What happens when T = 10.0 and T = 0.7? Why?**\n",
        "\n",
        "When the temperature is set very high like T = 10, the teacher's probability outputs become extremely soft, almost uniform. This means the model is unsure, and although the student sees a lot of relational information, the signal can become too weak. Training becomes slower and sometimes less accurate because the student cannot clearly identify which class should be favored.\n",
        "\n",
        "On the other hand, when the temperature is too low like T = 0.7, the distribution becomes very sharp, closely resembling one-hot labels. The student learns faster at first, but it loses access to the subtle inter-class similarities that make distillation beneficial in the first place.\n",
        "\n",
        "Through both experimentation and the literature, a temperature between 3 and 5 usually gives the ideal balance: the student receives enough relational information to learn effectively, without the signal becoming too flat or too weak.\n",
        "\n",
        "Different tasks may benefit from different temperature values. For fine-grained classification tasks like distinguishing between similar dog breeds, higher temperatures like 5 to 7 can be more beneficial because the inter-class relationships are especially important. For classification tasks with very distinct classes, lower temperatures like T 2 to 4, may be enough since the classes are already well separated."
      ],
      "metadata": {
        "id": "yjTXam_FtIcN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I also wanted to mention the importance of the alpha parameter in the distillation loss function, since alpha controls the balance between two components:\n",
        "\n",
        "- Hard loss (a): Cross-entropy with true labels, it ensures the student learns the correct classifications.\n",
        "- Soft loss (1-a): KL divergence with teacher's soft targets, it transfers the teacher's knowledge.\n",
        "\n",
        "The typical value is a = 0.5, giving equal weight to both objectives. However, this can be tuned based on the task. A higher a like 0.7 gives more emphasis on correct classification, which is useful when the teacher's model accuracy is not very high. On the other hand, a lower a like 0.3 gives more emphasis on mimicking the teacher, and it can be used when the teacher is very accurate and we want to maximize the knowledge transfer,\n",
        "\n",
        "In the code a=0.5 since it provides a balanced approach that allows the student model to learn the correct labels and benefit from the teacher's soft probability distributions."
      ],
      "metadata": {
        "id": "CALNHEMb6Hca"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall, knowledge distillation only works as intended when the teacher provides soft probabilities, the student uses log-probabilities, and the KL loss is scaled correctly using the temperature. When these components are in place, the student model can effectively learn from the teacher's knowledge while maintaining significantly lower computational costs. Success depends on tuning the temperature and alpha, according to the task requirements and the teacher model quality."
      ],
      "metadata": {
        "id": "-xSEKK8IzYiX"
      }
    }
  ]
}