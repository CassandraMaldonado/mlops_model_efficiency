# Model Efficiency

This repository contains experiments focused on improving model efficiency for deployment by using Knowledge Distillation and Quantization techniques.

Overview

The project explores how to make deep learning models smaller and faster without losing much accuracy.
Two main approaches were implemented and tested:
	1.	Knowledge Distillation (KD) — training a smaller student model using the output distribution from a larger, well-trained teacher model.
	2.	Quantization — converting floating-point models into lower-precision representations (such as INT8) to reduce memory footprint and inference latency.

The experiments were conducted on CIFAR-10 using ResNet architectures, implemented in PyTorch.

**1. Knowledge Distillation**


⸻

**2. Quantization**

